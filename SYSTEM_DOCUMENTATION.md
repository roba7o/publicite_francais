# French Article Scraper - Complete System Documentation

> [!abstract] System Overview
> A modular French news article scraper designed for language learning and vocabulary analysis. Supports multiple sources, concurrent processing, and structured output.

## Table of Contents
- [[#Overview|Overview]]
- [[#Architecture Diagram|Architecture Diagram]]  
- [[#Directory Structure|Directory Structure]]
- [[#System Flow|System Flow]]
- [[#Core Components|Core Components]]
- [[#Configuration System|Configuration System]]
- [[#Data Flow|Data Flow]]
- [[#Testing Framework|Testing Framework]]
- [[#Development Workflow|Development Workflow]]
- [[#Adding New Features|Adding New Features]]
- [[#Troubleshooting|Troubleshooting]]
- [[#Performance Considerations|Performance Considerations]]

## Overview

This is a **French Article Scraper** system designed to collect French news articles from multiple sources, process them for vocabulary learning, and output structured data. The system is built for language learners who want to analyze French text patterns and word frequencies.

> [!tip] Key Features
> - **Multi-source scraping**: Supports multiple French news websites
> - **Concurrent processing**: Handles multiple sources simultaneously  
> - **Offline testing**: Can work with local test data
> - **Word frequency analysis**: Extracts vocabulary with context
> - **CSV export**: Structured output for further analysis
> - **Robust error handling**: Graceful degradation when sources fail

> [!note] Target Use Case
> Language learners who want to:
> 1. Collect authentic French content
> 2. Analyze word frequency patterns  
> 3. Build vocabulary with real context
> 4. Track language learning progress

---

## Architecture Diagram

```mermaid
graph TD
    A[Config Files] --> B[Main Entry Point]
    C[Test Data] --> B
    B --> D[Core Processor]
    D --> E[Processing Pipeline]
    E --> F[Output Layer]
    F --> G[Output Files]
    
    subgraph "Config Files"
        A1[settings.py]
        A2[scrapers.py] 
        A3[website urls]
    end
    
    subgraph "Processing Pipeline"
        E1[Scrapers] --> E2[Parsers]
        E2 --> E3[Text Processors]
        E4[Slate.fr]
        E5[TF1 Info]
        E6[FranceInfo]
        E7[Depeche]
    end
    
    subgraph "Output Layer"
        F1[CSV Writer]
        F2[Validator]
        F3[Logger]
    end
```

> [!info] Architecture Overview
> The system follows a modular pipeline architecture with clear separation of concerns:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   CONFIG FILES  â”‚    â”‚   MAIN ENTRY    â”‚    â”‚   TEST DATA     â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚ â€¢ settings.py   â”‚â”€â”€â”€â–¶â”‚   main.py       â”‚â—€â”€â”€â”€â”‚ â€¢ test_data/    â”‚
â”‚ â€¢ scrapers.py   â”‚    â”‚                 â”‚    â”‚ â€¢ fixtures/     â”‚
â”‚ â€¢ website urls  â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚ â€¢ offline mode  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ â”‚OFFLINE/LIVE â”‚ â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚ â”‚   SWITCH    â”‚ â”‚
                       â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   CORE PROCESSOR                                â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚   IMPORT    â”‚    â”‚  PROCESS    â”‚    â”‚    SAVE     â”‚        â”‚
â”‚  â”‚   CLASSES   â”‚â”€â”€â”€â–¶â”‚   SOURCES   â”‚â”€â”€â”€â–¶â”‚   RESULTS   â”‚        â”‚
â”‚  â”‚             â”‚    â”‚             â”‚    â”‚             â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  PROCESSING PIPELINE                            â”‚
â”‚                                                                 â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚ â”‚   SCRAPERS   â”‚   â”‚   PARSERS    â”‚   â”‚  PROCESSORS  â”‚        â”‚
â”‚ â”‚              â”‚   â”‚              â”‚   â”‚              â”‚        â”‚
â”‚ â”‚ â€¢ Slate.fr   â”‚â”€â”€â–¶â”‚ â€¢ Extract    â”‚â”€â”€â–¶â”‚ â€¢ Clean text â”‚        â”‚
â”‚ â”‚ â€¢ TF1 Info   â”‚   â”‚   titles     â”‚   â”‚ â€¢ Tokenize   â”‚        â”‚
â”‚ â”‚ â€¢ FranceInfo â”‚   â”‚ â€¢ Extract    â”‚   â”‚ â€¢ Count freq â”‚        â”‚
â”‚ â”‚ â€¢ Depeche    â”‚   â”‚   content    â”‚   â”‚ â€¢ Add contextâ”‚        â”‚
â”‚ â”‚              â”‚   â”‚ â€¢ Extract    â”‚   â”‚              â”‚        â”‚
â”‚ â”‚ (concurrent) â”‚   â”‚   metadata   â”‚   â”‚              â”‚        â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     OUTPUT LAYER                                â”‚
â”‚                                                                 â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚ â”‚ CSV WRITER   â”‚   â”‚  VALIDATOR   â”‚   â”‚   LOGGER     â”‚        â”‚
â”‚ â”‚              â”‚   â”‚              â”‚   â”‚              â”‚        â”‚
â”‚ â”‚ â€¢ Daily filesâ”‚   â”‚ â€¢ Data check â”‚   â”‚ â€¢ Structured â”‚        â”‚
â”‚ â”‚ â€¢ Word rows  â”‚   â”‚ â€¢ URL check  â”‚   â”‚ â€¢ Debug info â”‚        â”‚
â”‚ â”‚ â€¢ Contexts   â”‚   â”‚ â€¢ Text check â”‚   â”‚ â€¢ Error trackâ”‚        â”‚
â”‚ â”‚ â€¢ Thread-safeâ”‚   â”‚              â”‚   â”‚              â”‚        â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                              â”‚
                              â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ OUTPUT FILES    â”‚
                    â”‚                 â”‚
                    â”‚ â€¢ YYYY-MM-DD.csvâ”‚
                    â”‚ â€¢ logs/         â”‚
                    â”‚ â€¢ debug info    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Directory Structure

> [!example]- File Tree Structure
> ```
> publicite_francais/
> â”œâ”€â”€ ðŸ“ src/                          # Main source code
> â”‚   â”œâ”€â”€ ðŸ“ config/                   # Configuration files
> â”‚   â”‚   â”œâ”€â”€ settings.py              # Global settings (OFFLINE, DEBUG)
> â”‚   â”‚   â””â”€â”€ website_parser_scrapers_config.py  # Source configurations
> â”‚   â”‚
> â”‚   â”œâ”€â”€ ðŸ“ core/                     # Core business logic
> â”‚   â”‚   â””â”€â”€ processor.py             # Main orchestrator class
> â”‚   â”‚
> â”‚   â”œâ”€â”€ ðŸ“ scrapers/                 # URL discovery components
> â”‚   â”‚   â”œâ”€â”€ slate_fr_scraper.py      # Slate.fr URL extractor
> â”‚   â”‚   â”œâ”€â”€ france_info_scraper.py   # FranceInfo URL extractor
> â”‚   â”‚   â”œâ”€â”€ tf1_info_scraper.py      # TF1 Info URL extractor
> â”‚   â”‚   â””â”€â”€ ladepeche_fr_scraper.py  # Depeche URL extractor
> â”‚   â”‚
> â”‚   â”œâ”€â”€ ðŸ“ parsers/                  # Content extraction components
> â”‚   â”‚   â”œâ”€â”€ base_parser.py           # Abstract base parser class
> â”‚   â”‚   â”œâ”€â”€ slate_fr_parser.py       # Slate.fr content extractor
> â”‚   â”‚   â”œâ”€â”€ france_info_parser.py    # FranceInfo content extractor
> â”‚   â”‚   â”œâ”€â”€ tf1_info_parser.py       # TF1 Info content extractor
> â”‚   â”‚   â””â”€â”€ ladepeche_fr_parser.py   # Depeche content extractor
> â”‚   â”‚
> â”‚   â”œâ”€â”€ ðŸ“ utils/                    # Utility modules
> â”‚   â”‚   â”œâ”€â”€ csv_writer.py            # CSV output handler
> â”‚   â”‚   â”œâ”€â”€ french_text_processor.py # Text processing & analysis
> â”‚   â”‚   â”œâ”€â”€ structured_logger.py     # Logging system
> â”‚   â”‚   â””â”€â”€ validators.py            # Data validation
> â”‚   â”‚
> â”‚   â”œâ”€â”€ ðŸ“ test_data/               # Test data for offline mode
> â”‚   â”‚   â””â”€â”€ raw_url_soup/           # Cached HTML files
> â”‚   â”‚       â”œâ”€â”€ slate_fr/           # Slate.fr test files
> â”‚   â”‚       â”œâ”€â”€ france_info/        # FranceInfo test files
> â”‚   â”‚       â”œâ”€â”€ tf1_fr/             # TF1 test files
> â”‚   â”‚       â””â”€â”€ depeche_fr/         # Depeche test files
> â”‚   â”‚
> â”‚   â”œâ”€â”€ ðŸ“ output/                  # Generated output files
> â”‚   â””â”€â”€ ðŸ“ logs/                    # System logs
> â”‚
> â”œâ”€â”€ ðŸ“ tests/                       # Test suite
> â”‚   â”œâ”€â”€ ðŸ“ integration/             # Integration tests
> â”‚   â”‚   â”œâ”€â”€ test_basic_functionality.py
> â”‚   â”‚   â””â”€â”€ test_offline_mode.py
> â”‚   â”œâ”€â”€ ðŸ“ fixtures/                # Test fixtures
> â”‚   â”œâ”€â”€ test_essential.py           # Core functionality tests
> â”‚   â””â”€â”€ conftest.py                 # Pytest configuration
> â”‚
> â”œâ”€â”€ ðŸ“ .github/                     # GitHub automation
> â”‚   â””â”€â”€ workflows/
> â”‚       â””â”€â”€ tests.yml               # CI/CD pipeline
> â”‚
> â”œâ”€â”€ Makefile                        # Development commands
> â”œâ”€â”€ requirements.txt                # Python dependencies
> â”œâ”€â”€ Dockerfile                      # Container configuration
> â””â”€â”€ SYSTEM_DOCUMENTATION.md         # This file
> ```

---

## System Flow

> [!summary] High-Level Flow
> ```mermaid
> flowchart LR
>     A[START] --> B[Load Config]
>     B --> C[Initialize Components]
>     C --> D[Process Sources]
>     D --> E[Save Results]
>     E --> F[END]
> ```

> [!example]- Detailed Processing Flow
> ```
> 1. INITIALIZATION
>    â”œâ”€â”€ Load settings (OFFLINE mode, DEBUG flags)
>    â”œâ”€â”€ Load scraper configurations
>    â””â”€â”€ Setup logging
> 
> 2. COMPONENT CREATION (for each source)
>    â”œâ”€â”€ Import scraper class dynamically
>    â”œâ”€â”€ Import parser class dynamically
>    â”œâ”€â”€ Create instances with configuration
>    â””â”€â”€ Handle import errors gracefully
> 
> 3. CONTENT ACQUISITION
>    â”œâ”€â”€ IF OFFLINE MODE:
>    â”‚   â””â”€â”€ Load cached HTML files from test_data/
>    â””â”€â”€ IF LIVE MODE:
>        â”œâ”€â”€ Scraper discovers article URLs
>        â”œâ”€â”€ Fetch HTML content from URLs
>        â””â”€â”€ Handle network errors
> 
> 4. CONTENT PROCESSING (concurrent)
>    â”œâ”€â”€ Parser extracts article content
>    â”œâ”€â”€ Text processor cleans and tokenizes
>    â”œâ”€â”€ Calculate word frequencies
>    â””â”€â”€ Extract word contexts
> 
> 5. OUTPUT GENERATION
>    â”œâ”€â”€ Validate processed data
>    â”œâ”€â”€ Write to daily CSV file
>    â””â”€â”€ Log processing statistics
> 
> 6. ERROR HANDLING (throughout)
>    â”œâ”€â”€ Log errors with context
>    â”œâ”€â”€ Continue processing other sources
>    â””â”€â”€ Graceful degradation
> ```

---

## Core Components

### 1. Main Entry Point (`src/main.py`)

**Purpose**: Orchestrates the entire system execution.

**Key Functions**:
```python
def main():
    # 1. Setup logging
    # 2. Load configurations
    # 3. Process all enabled sources concurrently
    # 4. Log final statistics
```

**How it works**:
1. Determines if running in OFFLINE or LIVE mode
2. Loads all scraper configurations
3. Processes each enabled source using ThreadPoolExecutor
4. Aggregates and logs final results

### 2. Core Processor (`src/core/processor.py`)

**Purpose**: Central orchestrator that manages the entire pipeline for each news source.

**Key Methods**:
```python
class ArticleProcessor:
    @staticmethod
    def import_class(class_path: str) -> type:
        # Dynamically imports classes from string paths
        
    @classmethod  
    def process_source(cls, config: ScraperConfig) -> Tuple[int, int]:
        # Main processing pipeline for a single source
        # Returns (processed_count, attempted_count)
```

**Processing Steps**:
1. **Dynamic Import**: Loads scraper and parser classes from configuration
2. **Content Acquisition**: Gets URLs (live) or files (offline)
3. **Processing Pipeline**: Extracts and processes article content
4. **Output**: Saves results to CSV
5. **Error Handling**: Logs errors, continues processing

### 3. Scrapers (`src/scrapers/`)

**Purpose**: Discover article URLs from news website homepages.

**Base Pattern**:
```python
class NewsSourceScraper:
    def __init__(self, debug=None):
        self.logger = get_structured_logger(self.__class__.__name__)
        self.base_url = "https://example.com/"
        self.headers = {"User-Agent": "..."}
        
    def get_article_urls(self, max_articles=8) -> List[str]:
        # 1. Fetch homepage HTML
        # 2. Parse for article links
        # 3. Return list of URLs
```

**Current Scrapers**:
- **SlateFrURLScraper**: Extracts from Slate.fr homepage
- **FranceInfoURLScraper**: Extracts from FranceInfo homepage  
- **TF1InfoURLScraper**: Extracts from TF1 Info homepage
- **LadepecheFrURLScraper**: Extracts from Depeche homepage

### 4. Parsers (`src/parsers/`)

**Purpose**: Extract article content (title, text, date) from individual article pages.

**Base Pattern**:
```python
class NewsSourceParser(BaseParser):
    def parse_article(self, soup: BeautifulSoup) -> Optional[Dict[str, Any]]:
        # 1. Extract article title
        # 2. Extract article content
        # 3. Extract publication date
        # 4. Return structured data
        
        return {
            "title": title,
            "full_text": content, 
            "article_date": date,
            "date_scraped": datetime.now().strftime("%Y-%m-%d"),
            "num_paragraphs": len(paragraphs)
        }
```

### 5. Text Processor (`src/utils/french_text_processor.py`)

**Purpose**: Processes French text for vocabulary analysis.

**Key Functions**:
```python
class FrenchTextProcessor:
    def process(self, text: str, top_n: int = 10) -> Dict[str, Any]:
        # Complete processing pipeline
        
    def clean_text(self, text: str) -> str:
        # Remove extra whitespace, normalize
        
    def tokenize(self, text: str) -> List[str]:
        # Split into words, lowercase, remove punctuation
        
    def remove_stopwords(self, tokens: List[str]) -> List[str]:
        # Filter out common French words
        
    def count_words(self, tokens: List[str]) -> Dict[str, int]:
        # Calculate word frequencies
```

### 6. CSV Writer (`src/utils/csv_writer.py`)

**Purpose**: Thread-safe CSV output for vocabulary data.

**Key Features**:
- **Daily files**: Creates files named `YYYY-MM-DD.csv`
- **Word-based rows**: Each row represents one word occurrence
- **Duplicate detection**: Prevents re-processing same articles
- **Thread safety**: Safe for concurrent access

**CSV Format**:
```csv
word,context,source,article_date,scraped_date,title,frequency
bonjour,"bonjour le monde",slate.fr,2025-07-14,2025-07-14,"Article Title",3
monde,"bonjour le monde",slate.fr,2025-07-14,2025-07-14,"Article Title",2
```

---

## Configuration System

### 1. Global Settings (`src/config/settings.py`)

**Purpose**: System-wide configuration flags.

```python
# Core settings
OFFLINE = True  # Use test data instead of live scraping
DEBUG = True    # Enable detailed logging

# Paths
OUTPUT_DIR = "output"
LOG_DIR = "logs"
```

### 2. Source Configuration (`src/config/website_parser_scrapers_config.py`)

**Purpose**: Defines which news sources to process and how.

```python
@dataclass
class ScraperConfig:
    name: str                    # Display name
    enabled: bool               # Whether to process
    scraper_class: str          # Full path to scraper class
    parser_class: str           # Full path to parser class
    scraper_kwargs: Optional[Dict] = None
    parser_kwargs: Optional[Dict] = None

SCRAPER_CONFIGS = [
    ScraperConfig(
        name="Slate.fr",
        enabled=True,
        scraper_class="scrapers.slate_fr_scraper.SlateFrURLScraper",
        parser_class="parsers.slate_fr_parser.SlateFrArticleParser",
        scraper_kwargs={"debug": True},
    ),
    # ... more sources
]
```

---

## Data Flow

### Input Data Flow
```
LIVE MODE:
Website Homepage â”€â”€â–¶ Scraper â”€â”€â–¶ Article URLs â”€â”€â–¶ HTTP Fetch â”€â”€â–¶ HTML Content

OFFLINE MODE:  
Test Data Files â”€â”€â–¶ Local File Read â”€â”€â–¶ HTML Content
```

### Processing Data Flow
```
HTML Content â”€â”€â–¶ Parser â”€â”€â–¶ Article Data â”€â”€â–¶ Text Processor â”€â”€â–¶ Word Analysis
                    â”‚              â”‚                â”‚               â”‚
                    â–¼              â–¼                â–¼               â–¼
                 Title         Full Text        Clean Text     Word Frequencies
                 Date          Metadata         Tokens         Contexts
```

### Output Data Flow
```
Word Analysis â”€â”€â–¶ Validator â”€â”€â–¶ CSV Writer â”€â”€â–¶ Daily CSV File
      â”‚                           â”‚               â”‚
      â–¼                           â–¼               â–¼
  Structured                Thread-Safe       YYYY-MM-DD.csv
  Word Data                  File Write       with word rows
```

---

## Testing Framework

### Test Structure
```
tests/
â”œâ”€â”€ test_essential.py           # 9 core functionality tests
â”œâ”€â”€ integration/
â”‚   â”œâ”€â”€ test_basic_functionality.py  # 8 integration tests
â”‚   â””â”€â”€ test_offline_mode.py         # 6 offline mode tests
â””â”€â”€ conftest.py                 # Shared fixtures
```

### Test Categories

**1. Essential Tests** (9 tests - fast)
- Component initialization
- Configuration loading  
- Basic functionality
- Import validation

**2. Integration Tests** (8 tests - comprehensive)
- End-to-end pipelines
- Component interactions
- Directory structure
- Concurrent processing

**3. Offline Mode Tests** (6 tests - critical)
- **Equivalent to `make run-offline`**
- System health validation
- Error handling
- Performance checks

### Running Tests
```bash
make tests           # All 23 tests (~1 minute)
make test-essential  # Core 9 tests (~2 seconds)  
make test-integration # Integration tests (~50 seconds)
make test-offline    # Offline mode test (~10 seconds)
```

---

## Development Workflow

### Daily Development
```bash
# 1. Make code changes
vim src/some_file.py

# 2. Quick test
make test-essential

# 3. Full validation  
make test-offline

# 4. Commit changes
git add . && git commit -m "Description"

# 5. Push (triggers GitHub Actions)
git push
```

### Available Commands
```bash
# Execution
make run             # Run with current settings
make run-live        # Force live mode
make run-offline     # Force offline mode

# Testing  
make tests           # Run all tests
make test-essential  # Quick smoke test
make test-offline    # Test offline functionality

# Code Quality
make lint            # Check code style
make format          # Auto-format code
make clean           # Remove cache files

# Docker
make docker-build    # Build container
make docker-run      # Run in container
```

### GitHub Actions
**Automatic CI/CD** runs on every commit:
1. **Matrix testing**: Python 3.11 & 3.12
2. **Full test suite**: All 23 tests
3. **Code linting**: flake8 checks
4. **Build validation**: Ensure system works

---

## Adding New Features

### Adding a New News Source

**1. Create Scraper** (`src/scrapers/new_source_scraper.py`)
```python
class NewSourceURLScraper:
    def __init__(self, debug=None):
        self.logger = get_structured_logger(self.__class__.__name__)
        self.base_url = "https://newsource.com/"
        self.headers = {"User-Agent": "..."}
        
    def get_article_urls(self, max_articles=8) -> List[str]:
        # 1. Fetch homepage
        response = requests.get(self.base_url, headers=self.headers)
        soup = BeautifulSoup(response.content, "html.parser")
        
        # 2. Find article links (customize selector)
        articles = soup.find_all("article")
        urls = []
        for article in articles:
            link = article.find("a")
            if link and link.has_attr("href"):
                url = urljoin(self.base_url, link["href"])
                urls.append(url)
                
        return urls[:max_articles]
```

**2. Create Parser** (`src/parsers/new_source_parser.py`)
```python
from parsers.base_parser import BaseParser

class NewSourceArticleParser(BaseParser):
    def __init__(self):
        super().__init__(site_domain="newsource.com")
        
    def parse_article(self, soup: BeautifulSoup) -> Optional[Dict[str, Any]]:
        try:
            # Extract title (customize selector)
            title_tag = soup.find("h1", class_="article-title")
            title = title_tag.get_text(strip=True) if title_tag else "Unknown"
            
            # Extract content (customize selector) 
            content_div = soup.find("div", class_="article-content")
            if not content_div:
                return None
                
            paragraphs = [p.get_text(strip=True) 
                         for p in content_div.find_all("p")]
            full_text = "\n\n".join(paragraphs)
            
            # Extract date (customize selector)
            date_tag = soup.find("time")
            date = self._parse_date(date_tag) if date_tag else datetime.now().strftime("%Y-%m-%d")
            
            return {
                "title": title,
                "full_text": full_text, 
                "article_date": date,
                "date_scraped": datetime.now().strftime("%Y-%m-%d"),
                "num_paragraphs": len(paragraphs)
            }
            
        except Exception as e:
            self.logger.error(f"Error parsing article: {e}")
            return None
```

**3. Add to Configuration** (`src/config/website_parser_scrapers_config.py`)
```python
SCRAPER_CONFIGS = [
    # ... existing sources ...
    ScraperConfig(
        name="New Source",
        enabled=True,
        scraper_class="scrapers.new_source_scraper.NewSourceURLScraper",
        parser_class="parsers.new_source_parser.NewSourceArticleParser",
        scraper_kwargs={"debug": True},
    ),
]
```

**4. Test the New Source**
```bash
# Test with your new source
make test-offline

# Check logs for any errors
cat src/logs/latest.log
```

### Adding New Text Processing Features

**Extend FrenchTextProcessor** (`src/utils/french_text_processor.py`)
```python
class FrenchTextProcessor:
    def extract_adjectives(self, tokens: List[str]) -> List[str]:
        """Extract French adjectives from tokens."""
        # Add your implementation
        pass
        
    def find_conjugated_verbs(self, tokens: List[str]) -> Dict[str, List[str]]:
        """Find verb conjugations in text."""
        # Add your implementation  
        pass
        
    def process(self, text: str, top_n: int = 10) -> Dict[str, Any]:
        # Extend existing method
        result = super().process(text, top_n)
        
        # Add your new features
        result["adjectives"] = self.extract_adjectives(tokens)
        result["verbs"] = self.find_conjugated_verbs(tokens)
        
        return result
```

### Adding New Output Formats

**Create New Writer** (`src/utils/json_writer.py`)
```python
class JSONWriter:
    def __init__(self, output_dir="output"):
        self.output_dir = output_dir
        self.logger = get_structured_logger(self.__class__.__name__)
        
    def write_article(self, parsed_data: dict, url: str, word_data: dict):
        """Write article data to JSON format."""
        filename = f"{datetime.now().strftime('%Y-%m-%d')}.json"
        filepath = os.path.join(self.output_dir, filename)
        
        # Your JSON output logic here
        pass
```

**Integrate in Core Processor** (`src/core/processor.py`)
```python
from utils.json_writer import JSONWriter

class ArticleProcessor:
    @classmethod
    def process_source(cls, config: ScraperConfig) -> Tuple[int, int]:
        # ... existing code ...
        
        # Add additional output formats
        csv_writer = DailyCSVWriter()
        json_writer = JSONWriter()  # New format
        
        # Save in multiple formats
        csv_writer.write_article(parsed_data, url, word_freqs, word_contexts)
        json_writer.write_article(parsed_data, url, word_data)
```

---

## Troubleshooting

### Common Issues

**1. "Module not found" errors**
```bash
# Check PYTHONPATH
echo $PYTHONPATH

# Run from project root with correct path
PYTHONPATH=src python -m main
```

**2. Network errors in live mode**
```bash
# Switch to offline mode for testing
make run-offline

# Or set environment variable
OFFLINE=True make run
```

**3. Empty output files**
```bash
# Check if sources are enabled
grep "enabled.*True" src/config/website_parser_scrapers_config.py

# Check logs for errors
tail -f src/logs/latest.log
```

**4. Test failures**
```bash
# Run specific test category
make test-essential

# Check test output for specific failures
make test-offline -v
```

### Debug Mode

**Enable detailed logging**:
```python
# In src/config/settings.py
DEBUG = True
```

**Check logs**:
```bash
# View latest logs
tail -f src/logs/latest.log

# Search for errors
grep -i error src/logs/*.log
```

### Performance Issues

**1. Slow processing**
- Check if running too many sources concurrently
- Verify network connectivity in live mode
- Consider reducing `max_articles` parameter

**2. Memory usage**
- Large text processing can use significant memory
- Monitor with `top` or `htop` during execution
- Consider processing smaller batches

---

## Performance Considerations

### System Limits
- **Concurrent sources**: 4 sources process simultaneously
- **Articles per source**: 8 articles maximum (configurable)
- **Request delays**: Built-in delays prevent rate limiting
- **Memory usage**: ~50-100MB typical, ~200MB peak

### Optimization Tips

**1. Offline Mode for Development**
```bash
# Use offline mode for faster iteration
make run-offline  # ~10 seconds vs ~60 seconds live
```

**2. Selective Source Processing**
```python
# Disable sources in config for testing
ScraperConfig(
    name="Heavy Source",
    enabled=False,  # Temporarily disable
    # ...
)
```

**3. Reduced Article Counts**
```python
# In scraper initialization
def get_article_urls(self, max_articles=3):  # Reduce from 8
```

**4. Logging Level Control**
```python
# In src/config/settings.py
DEBUG = False  # Reduce log verbosity for production
```

### Monitoring

**Resource Usage**:
```bash
# Monitor during execution
watch -n 1 'ps aux | grep python'

# Check output file sizes
ls -lh src/output/
```

**Processing Statistics**:
- Logged at end of each run
- Shows articles processed per source
- Includes timing information
- Available in log files

---

## Conclusion

This French Article Scraper is a **modular, extensible system** designed for:
- **Language learners** who want authentic French content
- **Developers** who need a robust web scraping framework
- **Researchers** who want to analyze French text patterns

The architecture supports easy extension with new sources, processing algorithms, and output formats while maintaining reliability through comprehensive testing and error handling.

**Key Strengths**:
- âœ… **Modular design**: Easy to extend and modify
- âœ… **Robust error handling**: Graceful degradation
- âœ… **Comprehensive testing**: 23 tests with CI/CD
- âœ… **Offline development**: Fast iteration with test data
- âœ… **Clear documentation**: This guide helps you understand everything

**Next Steps**:
1. **Explore the code**: Start with `src/main.py` and follow the flow
2. **Run tests**: `make tests` to see everything working
3. **Try offline mode**: `make run-offline` for safe experimentation
4. **Add a new source**: Follow the guide above
5. **Customize processing**: Extend text analysis features

Happy coding! ðŸš€